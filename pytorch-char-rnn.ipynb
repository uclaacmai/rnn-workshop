{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions in Machine Learning\n",
    "\n",
    "Every machine learning model has assumptions, indeed, that is the only way we can reasonably learn from data and make assumptions about how what we learn generalizes to the real world. \n",
    "\n",
    "The problem is the assumptions/difficulty tradeoff. Simpler models tend to have more naive assumptions (ie, assumptions that are not likely to be true or not likely to generalize), and more complex models (such as neural networks, convolutional networks, and recurrent neural networks) have less assumptions (and therefore, save for overfitting, are more likely to generalize better to the real world), difficult (and expensive) to build, train, and deploy. \n",
    "\n",
    "To understand this specifically, lets go through some models and state the assumptions they make. \n",
    "\n",
    "\n",
    "### Linear Regression & Logistic Regression\n",
    "\n",
    "Linear regression assumes that the data can be generated via a linear function, and logistic regression assumes that the data can be linearly separated. \n",
    "\n",
    "![lin](http://sebastianraschka.com/images/blog/2014/kernel_pca/linear_vs_nonlinear.png)\n",
    "\n",
    "Although we may intuitively think that assuming linearity is a very naive assumption, these are two of the most common algorithms used in the industry to make reasonable predictions for the following reasons: \n",
    "\n",
    "1. They are relatively easy to train, and don't require expensive & difficult research to optimize\n",
    "2. They have well known theoretical gaurantees and have been shown to work well in practice for decades\n",
    "3. Domain expertise can be applied to make sense of what has been learned\n",
    "4. The assumptions often turn out to be mostly true, and the tradeoff we have to make to get the edge cases classified correctly aren't worth the extra work.\n",
    "\n",
    "### Naive Bayes Classifier\n",
    "\n",
    "Bayes Rule states that for a latent variable and an evidence variable, the posterior distribution of the latent variable is equivalent to the prior probability of the latent variable times the likelihood of the evidence given the latent variable, divided by a constant (the probability of observing the evidence). \n",
    "\n",
    "Specifically:\n",
    "\n",
    "$$ p(y_i | x) = \\frac{p(y_i) p(x | y_i)}{p(x)} = \\frac{p(x, y_i)}{\\sum_i p(x, y_i)} $$\n",
    "\n",
    "Using the rules of probability, we can write the joint distribution as a conditional:\n",
    "\n",
    "$$ p(x, y_i) = p(x_1 | x_2, ... x_n, y_i) * p(x_2 | x_3, ... x_n, y_i) $$\n",
    "\n",
    "Here is where the assumption comes into play: since we can't really compute these conditional probabilities, we just define: \n",
    "\n",
    "$$ p(x_1 | x_2, ... x_n, y_i) = p(x_1 | y_i) $$\n",
    "\n",
    "This assumption means that each feature in our training set is entirely dependent upon the label that it is given, and is completely independent of every other feature, given the class label.\n",
    "\n",
    "We cal also set $ p(x_1 | y_i) $ to take on whatever probability distribution we want to (such as the Gaussian); doing this is also another assumption in our model. \n",
    "\n",
    "### Neural Networks\n",
    "- They also assume that the data are conditionally indpendent of each other. \n",
    "\n",
    "\n",
    "We have used Neural Networks and Convolutional Neural Netoworks to remove the assumptions of linearity in our data (and for convolutional neural networks, the assumption of spatial variance being important). Now we will use Recurrent Neural Networks to remove the assumption of independence (to an extent). \n",
    "\n",
    "Aside: A much simpler model (that doesn't work as well, but still has produced good results) that removes this independence assumption are Hidden Markov Models.\n",
    "\n",
    "\n",
    "\n",
    "### An Introduction to Recurrent Neural Networks in Tensorflow\n",
    "\n",
    "Recurrent Neural Networks are models that are able to model relationships between data where each input is not  independent of the previous inputs. \n",
    "\n",
    "For example, consider the field of Natural Language Processing (NLP). NLP deals with all tasks involved with processing texts and obtaining information (learning) from text. It is extremely common, especially today, to solve NLP tasks with machine learning (and especially deep learning). \n",
    "\n",
    "Recurrent Neural networks are a popular method for solving NLP problems, including predicting the sentiment of a movie review, predicting the next word (or character) in a sequence of words or characters, and even translating text to another language. This is because they can model dependencies, and often in text a current word is dependent on the previous words (you can probably think of several examples). \n",
    "\n",
    "Just for a break from all this technical talk, here is some Shakespeare: \n",
    "\n",
    "```\n",
    "PANDARUS:\n",
    "Alas, I think he shall be come approached and the day\n",
    "When little srain would be attain'd into being never fed,\n",
    "And who is but a chain and subjects of his death,\n",
    "I should not sleep.\n",
    "\n",
    "Second Senator:\n",
    "They are away this miseries, produced upon my soul,\n",
    "Breaking and strongly should be buried, when I perish\n",
    "The earth and thoughts of many states.\n",
    "\n",
    "DUKE VINCENTIO:\n",
    "Well, your wit is in the care of side and that.\n",
    "\n",
    "Second Lord:\n",
    "They would be ruled after this chamber, and\n",
    "my fair nues begun out of the fact, to be conveyed,\n",
    "Whose noble souls I'll have the heart of the wars.\n",
    "\n",
    "Clown:\n",
    "Come, sir, I will make did behold your worship.\n",
    "\n",
    "VIOLA:\n",
    "I'll drink it.\n",
    "```\n",
    "\n",
    "\n",
    "### The Power of Recurrent Neural Networks\n",
    "\n",
    "I lied, the above was not really Shakespeare. It was actually a poem written by a recurrent neural network that trained on a dataset of Shakespeare's writings. This should give you an idea of the power of RNNs: just by looking at characters (not even words!) it learned how to generate strikingly similar text to Shakespeare. Granted, it does not make much sense but it is likely that at least engineers will not be able to tell the difference.\n",
    "\n",
    "This was from an awesome article here http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "\n",
    "Recurrent Neural Networks are important because they model sequential data. Each neuron, in addition to its data input, receives a previous ** state ** which is just a vector of numbers describing some previous computation in the RNN. This state input is what allows RNNs to learn dependencies in sequential data. \n",
    "\n",
    "It's a lot similar to you reading this sentence - when you understand the meaning of a particular word, you don't start figuring it out from scratch, but take the previous words and sentences into account. \n",
    "\n",
    "Here is what an RNN looks like: \n",
    "\n",
    "![rnn](https://cdn-images-1.medium.com/max/1600/1*V2W4TCmTj2h1CE7I-DngPw.png)\n",
    "\n",
    "\n",
    "And this is what an RNN looks like, predicting words character by character: \n",
    "\n",
    "![rnn-2](https://cdn-images-1.medium.com/max/1600/1*IMalbwl6uj3nlqxixZYFvA.jpeg)\n",
    "\n",
    "To understand RNNs deeply, we'll need to get into some data and code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "- First, let's import everything we need. Download the file \"shakespeare.txt\" and ensure that it is placed in the subdirectory 'data'. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file = open('./data/shakespeare.txt').read()\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the RNN model. We'll use an LSTM cell for our hidden units. We'll also feed the model raw input that is converted into a character-level embedding (similar to word embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import autograd\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, *, input_size, embedding_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # map inputs to embeddings\n",
    "        self.embedding_layer = nn.Embedding(input_size, embedding_size)\n",
    "        # forward embeddings through LSTM\n",
    "        self.LSTM = nn.LSTM(embedding_size, hidden_size, n_layers)\n",
    "        # compute a linear transformation to output space\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        # input should be batch_size * input_dim\n",
    "        input = self.embedding_layer(input.view(1, -1))\n",
    "        # input into LSTM is expected to be seq_len * batch_size * input_shape\n",
    "        output, hidden = self.LSTM(input.view(1, 1, -1), hidden)\n",
    "        # output should be batch_size * input_shape\n",
    "        output = self.linear(output.view(1, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # need 2 hidden state inits: one for the hidden state and one for the cell state \n",
    "        # dim is layers, batch_size, hidden_size\n",
    "        return (autograd.Variable(torch.zeros(self.n_layers, 1, self.hidden_size)),\n",
    "                autograd.Variable(torch.zeros(self.n_layers, 1, self.hidden_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write some code to prepare our inputs to our model. We'll need to write a function to extract a sequence from our file, a function to convert it to a fixed-length vector that can be fed into our model, and an overarching function that generates pairs of inputs and labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "seq_len = 200\n",
    "\n",
    "def get_seq(seq_len = 200):\n",
    "    start = np.random.randint(0, file_len - seq_len)\n",
    "    seq = file[start:start + seq_len]\n",
    "    assert len(seq) == 200\n",
    "    return seq\n",
    "\n",
    "def to_vector(seq, chars = all_characters):\n",
    "    return autograd.Variable(torch.LongTensor([chars.index(s) for s in seq]))\n",
    "\n",
    "def generate_training_set():\n",
    "    seq = get_seq()\n",
    "    inputs, labels = to_vector(seq[:-1]), to_vector(seq[1:])\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's write a function to output example sequences of characters from our RNN. We'll use the outputs of the RNN as a probability distribution, and obtain the most likely next character by sampling. We'll also use the concept of \"priming\", which is building up the hidden state so that the RNN already has a representation of previous timestep knowledge before we start sampling from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = rnn.init_hidden()\n",
    "    prime_input = to_vector(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = rnn(prime_input[p], hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = rnn(inp, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = to_vector(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take our RNN, define our hyperparameters, and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 5000\n",
    "hidden_size = 200\n",
    "embedding_size = 100\n",
    "n_layers = 4\n",
    "        \n",
    "rnn = RNN(input_size = n_characters,\n",
    "          embedding_size = embedding_size,\n",
    "          hidden_size = hidden_size, \n",
    "          output_size = n_characters,\n",
    "          n_layers = n_layers)\n",
    "\n",
    "optim = torch.optim.Adam(rnn.parameters(), lr = 0.005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "all_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # re-init hidden and zero the grads\n",
    "    hidden = rnn.init_hidden()\n",
    "    rnn.zero_grad()\n",
    "    inputs, labels = generate_training_set()\n",
    "    # run through the inputs one by one, accumulating a loss\n",
    "    loss = 0\n",
    "    for c in range(seq_len - 1):\n",
    "        output, hidden = rnn(inputs[c], hidden)\n",
    "        loss += criterion(output, labels[c])\n",
    "    loss.backward(retain_graph = True)\n",
    "    optim.step()\n",
    "    all_losses.append(loss.data[0]/seq_len)\n",
    "    if epoch % 20 == 0:\n",
    "        print('EPOCH: {}'.format(epoch))\n",
    "        predicted = evaluate()\n",
    "        print('predicted: {}'.format(predicted))\n",
    "        print('LOSS: {}'.format(loss.data[0]/seq_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
