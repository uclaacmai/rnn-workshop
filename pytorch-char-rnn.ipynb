{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 1115393\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file = open('./data/shakespeare.txt').read()\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import autograd\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, *, input_size, embedding_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # map inputs to embeddings\n",
    "        self.embedding_layer = nn.Embedding(input_size, embedding_size)\n",
    "        # forward embeddings through LSTM\n",
    "        self.LSTM = nn.LSTM(embedding_size, hidden_size, n_layers)\n",
    "        # compute a linear transformation to output space\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        input = self.embedding_layer(input.view(1, -1))\n",
    "        output, hidden = self.LSTM(input.view(1, 1, -1), hidden)\n",
    "        output = self.linear(output.view(1, -1))\n",
    "        scores = F.softmax(output)\n",
    "        return output, hidden, scores\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (autograd.Variable(torch.zeros(self.n_layers, 1, self.hidden_size)),\n",
    "                autograd.Variable(torch.zeros(self.n_layers, 1, self.hidden_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "seq_len = 200\n",
    "\n",
    "def get_seq(seq_len = 200):\n",
    "    start = np.random.randint(0, file_len - seq_len)\n",
    "    seq = file[start:start + seq_len]\n",
    "    assert len(seq) == 200\n",
    "    return seq\n",
    "\n",
    "def to_vector(seq, chars = all_characters):\n",
    "    return autograd.Variable(torch.LongTensor([chars.index(s) for s in seq]))\n",
    "\n",
    "def generate_training_set():\n",
    "    seq = get_seq()\n",
    "    inputs, labels = to_vector(seq[:-1]), to_vector(seq[1:])\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(start_char='A', predict_len=100, temperature=0.8):\n",
    "    # initialize the hidden state\n",
    "    hidden = rnn.init_hidden()\n",
    "    # convert the starting string to vector\n",
    "    start_vector = to_vector(start_char)\n",
    "    predicted = start_char\n",
    "    input = start_vector[-1]\n",
    "    for p in range(predict_len):\n",
    "        # feed the input to the network\n",
    "        output, hidden, scores = rnn(input, hidden)\n",
    "        # Convert output to a multinomial and sample the most likely element\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_1_idx = torch.multinomial(output_dist, 1)[0]\n",
    "        # index into the list of all characters, and use this predicted character as the next input to the RNN\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_1_idx]\n",
    "        predicted += predicted_char\n",
    "        input = to_vector(predicted_char)\n",
    "\n",
    "    return predicted, other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel/__main__.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n",
      "predicted: A\n",
      "iDT6\":o8{@8w*X, Owl|t&>.0t\f",
      "-4Xs7h66#iV+r\t\n",
      "89VU+(I\\J-/GC:|.#Z;EZ]o}_@ Ri*OvB~s\n",
      "LOSS: 4.572554016113282\n",
      "EPOCH: 20\n",
      "predicted: AB8+b5Q{\"#_*{_MD\\FM*+}^91F\f",
      "\u000b",
      "\t\u000b",
      "h(~)sne g,kt\n",
      " hto \n",
      "  eoA;uuOo d teeeiehmhetotstroa e:st ae  ie:hf\n",
      " atan\n",
      "LOSS: 3.270144348144531\n",
      "EPOCH: 40\n",
      "predicted: AY:mie eene n,\n",
      " by r e\n",
      "i e te\n",
      "owoot ec te rnydalpahblTget \n",
      "c\n",
      " \n",
      "alnleh r brg e\n",
      "\n",
      "mo.c H r,l \n",
      "enesL anei\n",
      "LOSS: 3.266881408691406\n",
      "EPOCH: 60\n",
      "predicted: Aettef ca\n",
      "tgidtte,mdaltcA, Ney tAkntekI t  e\n",
      "iieehs rcott Iooto t  d.n  hailt t  nWi T\n",
      "y ele tsoo eoe\n",
      "LOSS: 3.165647277832031\n",
      "EPOCH: 80\n",
      "predicted: Apb:mfmla:l htn\n",
      "trFmos ehcioha woha uaitYsitNo aoegoob\n",
      "et h,ashxihulpu er\n",
      "r u,nhwogmetsbthb :t l ra n\n",
      "LOSS: 3.4818759155273438\n",
      "EPOCH: 100\n",
      "predicted: A6poS iretrh uytweselsntsm  trT ut koawyo ne\n",
      "seno s u fA nar\n",
      "ss.h ai\n",
      "aoii siloooaThrare  Etitm.kt  o\n",
      "\n",
      "LOSS: 3.1709375\n",
      "EPOCH: 120\n",
      "predicted: AZzrOreePo,hayaso ot ecns\n",
      " hdUW\n",
      "\n",
      "erihsd oemwi  \n",
      "\n",
      "eso InrOmh\n",
      "rO   ylonulentcrhr\n",
      "pidTyt.ee\n",
      "i\n",
      "L.,\n",
      "ts   t\n",
      "LOSS: 3.4831787109375\n",
      "EPOCH: 140\n",
      "predicted: AM\"t(uieicttt shoia  oe otSo t eutfhte se  ow t a fdiem  y tMene ee\n",
      "ddtuoha dln.oadaa tp sh ipdaet no\n",
      "LOSS: 3.1989849853515624\n",
      "EPOCH: 160\n",
      "predicted: A\tc oteet   tom asneeAIc\n",
      "rgI miuc cmI  orr :ogari \n",
      "l dao w oy ni euheHed olrn n :httacle  ntha !mT Yw\n",
      "LOSS: 3.346424255371094\n",
      "EPOCH: 180\n",
      "predicted: Alhft  our dl\n",
      "oouetlsdb \n",
      "omnt eyhrrewotes s nao t,lwsht\n",
      "a t ootrbde ttloy  oleteh  s cgu\n",
      "f ditrt  igd\n",
      "LOSS: 3.2486746215820315\n",
      "EPOCH: 200\n",
      "predicted: Ao '\n",
      " e  errie enhsrru d hohio\n",
      " uuB ro\n",
      "uuhfe's\n",
      "!Ur'\n",
      "Ih hht lel Oalio eotetWf! h ewirh  hetdeittru a s\n",
      "LOSS: 3.3440435791015624\n",
      "EPOCH: 220\n",
      "predicted: A}guo\n",
      "ee e  n ohBa\n",
      ",eeh.aI n oe nhe  snWWntr um  t   eestghoEayleL h\n",
      "YNhah N\n",
      " enaakmnt tnhoIeooA A t,\n",
      "LOSS: 3.5653897094726563\n",
      "EPOCH: 240\n",
      "predicted: AjNhlde,heh  sshea,sletht i ysttoraotil he raamdee\n",
      " \n",
      " idneoff ciehebItcri ieno \n",
      "eemy  gls rMts er\n",
      "fpe\n",
      "LOSS: 4.135047607421875\n",
      "EPOCH: 260\n",
      "predicted: A^ ubfyy y: th st iwoe:Ie    egdn a  r lheaenaaa dnleeshiem\n",
      "  hie aomohtha 'rhyosha o:wt yshot hreU e\n",
      "LOSS: 3.1978662109375\n",
      "EPOCH: 280\n",
      "predicted: Akua  aoeaylGot meh !eaeecis  er ham tcsn  oiaweiiorhoavom r ivs;ai irliu Sea kc\n",
      " ecn reteoisna  \n",
      "\n",
      "ae\n",
      "LOSS: 3.176607360839844\n",
      "EPOCH: 300\n",
      "predicted: A0O  ao:   y,stwhr t.wnfrakhn!y ttwo\n",
      "l  mas rI sy  i tu l\n",
      "n loeIo iftmeh a lao\n",
      "sNt  wesnf\n",
      ":w mtR,  eo\n",
      "LOSS: 3.3628173828125\n",
      "EPOCH: 320\n",
      "predicted: AsdshnoasenhdgtK  shptstAosh ra   w kp rrI\n",
      ", t twma sr e xt t sneoar tt oauesttyt eu t ,rs \n",
      " dl \n",
      "m le\n",
      "LOSS: 3.563395690917969\n",
      "EPOCH: 340\n",
      "predicted: Avnoireslh twertato teyt oA yhmoereof!eiinTuai el l yo jcls outt , ye ofis dr W  ' kejeosfeeeaoeeehdo\n",
      "LOSS: 3.1557257080078127\n",
      "EPOCH: 360\n",
      "predicted: ASUELONS: E n tat l,atN min I\n",
      "haeinah\n",
      "t S t t c taHas t  r t  ame it w ts oHihca\n",
      "NCEDL:nydoTt'x lhi t\n",
      "LOSS: 3.052909240722656\n",
      "EPOCH: 380\n",
      "predicted: A h - noso Io h lou'r g sosoish\n",
      "Sheat iFe Eot teovw mone\n",
      "Ihgeye fhfe\n",
      "eh uodosheehunos neau leonrotone\n",
      "LOSS: 2.941712646484375\n",
      "EPOCH: 400\n",
      "predicted: Ag-\n",
      "T:taa:athot rd  c rheot reumotr onir keh.ith \n",
      "Tt.e  oeenao ee motr d ner? f le tre lee t t, omanw\n",
      "LOSS: 2.8421661376953127\n",
      "EPOCH: 420\n",
      "predicted: A onh afobros's ririmhrh nh. kuoleseyet \n",
      "E MN:amouhhiw.ye Orn s Ete ur tororh Yet on p tr shofilyoob \n",
      "LOSS: 2.847825012207031\n",
      "EPOCH: 440\n",
      "predicted: AUTS:\n",
      "Aour b,er yn gtye t toot toash, vos r'e: sost \n",
      "O'hoyte\n",
      "O ilush\n",
      "\n",
      "YR:\n",
      "Wotl\n",
      "Sy ph sodoaperagh sl b\n",
      "LOSS: 2.6994287109375\n",
      "EPOCH: 460\n",
      "predicted: Asilh tos tn sil he'h we sh hne the,,ans hhete ne fusd anweenn We thall at isot dsam hh\n",
      "Iiwes hol bhe\n",
      "LOSS: 2.853055725097656\n",
      "EPOCH: 480\n",
      "predicted: AWKNO :\n",
      "Th or dac th l, hhe,l\n",
      "Wh khes mot teohe ke tha w, put kor.\n",
      "Wes\n",
      "\n",
      "GUAREVNLU:\n",
      "Ls\n",
      "\n",
      "LENEEIEe:\n",
      "Anod\n",
      "LOSS: 2.8317462158203126\n",
      "EPOCH: 500\n",
      "predicted: A:mor sofah anander an whh ahed Ihins\n",
      "Sase lo ghes yh fiteryceld,ert,es: anus, yhard be dev wo wt  na\n",
      "LOSS: 2.8180718994140626\n",
      "EPOCH: 520\n",
      "predicted: AAOANIAYE:\n",
      "Ihe yot ast br tarossineaus hu s:ulg to bocisd chen hi cyog hoat mas norc sil domge tootos\n",
      "LOSS: 2.6570358276367188\n",
      "EPOCH: 540\n",
      "predicted: AIENEE :\n",
      "Hasl der sore,use ne mov andosee sold onm int nolht\n",
      "whil deun ans mo hoteon, me Ie tom got m\n",
      "LOSS: 2.688279724121094\n",
      "EPOCH: 560\n",
      "predicted: AGR:\n",
      "Iisauge end oo, it ham ir lree ye fe cuer seal,.y whaus lou an mhenn tiv lise, yheed maalye wee \n",
      "LOSS: 2.6722216796875\n",
      "EPOCH: 580\n",
      "predicted: AF:\n",
      "Iam eom'v any. per turs mony waviar an,: hre heir,\n",
      "Yu foge teg hhard' for y tare horrs sordim and\n",
      "LOSS: 2.720224609375\n",
      "EPOCH: 600\n",
      "predicted: A:\n",
      "Iterit un, tol tot,\n",
      "Auw Bap ou tinssed he teov an toret atef tol bee arl d mone,\n",
      "We lre, we por,e \n",
      "LOSS: 2.788710632324219\n",
      "EPOCH: 620\n",
      "predicted: AF:\n",
      "Ainr Iagen Mase h tham tams sa piun ther hof\n",
      "Mo thare s nien,\n",
      "Tor tic anl,\n",
      "An Ravh ehh anl\n",
      "Hor ma\n",
      "LOSS: 2.578482360839844\n",
      "EPOCH: 640\n",
      "predicted: ACL:\n",
      "Ion s tar pth haltardes in, goaf in,e annh mo oow do loin okaf the:\n",
      "\n",
      "Fedl sruct fe in ter hom,\n",
      "\n",
      "\n",
      "LOSS: 2.5217575073242187\n",
      "EPOCH: 660\n",
      "predicted: A:h toes! move at tousees.\n",
      "Pne hher wen h ne?\n",
      "\n",
      "KRIGCIMW:\n",
      "Hit kord misdidrerges ir aund wor fate bre t\n",
      "LOSS: 2.483662109375\n",
      "EPOCH: 680\n",
      "predicted: ARNU:\n",
      "Is fo rill thauspn whee thole the wot ir ovhiyos\n",
      "in-to isls lot in hiw thath;\n",
      "\n",
      "Crasichaur to si\n",
      "LOSS: 2.8063348388671874\n",
      "EPOCH: 700\n",
      "predicted: A:. so thaite beud mome voute hers move mi.h tom cot As hirolter he but, th and th e that then wo hit\n",
      "LOSS: 2.491100006103516\n",
      "EPOCH: 720\n",
      "predicted: Atild: toas fit le coyont\n",
      "Bod;\n",
      "Tiseag we thal coule yeut be, to sing af inedk Is. werige O hous me Ia\n",
      "LOSS: 2.4663856506347654\n",
      "EPOCH: 740\n",
      "predicted: AYw'nod; wum miny af mes osut sor mithor qed sher on'd! Bers thy se th af the I he poc,\n",
      "Ang Als mive \n",
      "LOSS: 2.5777056884765623\n",
      "EPOCH: 760\n",
      "predicted: ADREGO:\n",
      "\n",
      "HRIC:\n",
      "Met \n",
      "CAATN:\n",
      "Ild\n",
      "Gyup bachise thosse goto, tavt mimeros by srat whab thouds.\n",
      "\n",
      "BARTENS:\n",
      "\n",
      "LOSS: 2.314805450439453\n",
      "EPOCH: 780\n",
      "predicted: A yot' thith\n",
      "Bont fo hout nant Iode on ovinediins he sere, sishenm nel hhithoweegl fes\n",
      "'n, to ry sith\n",
      "LOSS: 2.4077601623535156\n",
      "EPOCH: 800\n",
      "predicted: Ame f mit the ron told nas nhamy sou leyd bide bomes meord it az to yhe to yhee, in budrens: cot th m\n",
      "LOSS: 2.4920669555664063\n",
      "EPOCH: 820\n",
      "predicted: Aes thor word cat pourg an covf inced arch If yo doite the tonyl ofit tes ying Ris thas coard, e to t\n",
      "LOSS: 2.2967481994628907\n",
      "EPOCH: 840\n",
      "predicted: A \n",
      "ALAGNUWLEASE:\n",
      "Ad, brel anfy the a ine wel bemingy peeeg wyal sat and woulxele i ad fauke als.\n",
      "Ift \n",
      "LOSS: 2.3503370666503907\n",
      "EPOCH: 860\n",
      "predicted: Ageds und won enrerese bath be sung banris,\n",
      "Ang. Come fabd ne if, heaw hot touw to ses one,\n",
      "\n",
      "KDEEI UN\n",
      "LOSS: 2.2526789855957032\n",
      "EPOCH: 880\n",
      "predicted: AROY:\n",
      "Rour the are hukhey anse hume cou none come miur wouny onl ilsster the meedgen porn meal. af,hi\n",
      "LOSS: 2.1694248962402343\n",
      "EPOCH: 900\n",
      "predicted: AY OAENE:\n",
      "Roulelres? anp end, fe myages to thaual he he ce me whes sut og fe efe, to the ftinl,\n",
      "Wo te\n",
      "LOSS: 2.4913763427734374\n",
      "EPOCH: 920\n",
      "predicted: Af in soug\n",
      "Aul he lnamot\n",
      "An name you wirim ghoir wouluur thy I thin, to wonl, ghaitheinsele gone.\n",
      "Mal\n",
      "LOSS: 2.630662841796875\n",
      "EPOCH: 940\n",
      "predicted: Ary mpere in yilg,\n",
      "Ip and yout Rel on he at car ar sre cancor loor tane cas bur wlore wath.\n",
      "\n",
      "MONHEALE\n",
      "LOSS: 2.19514892578125\n",
      "EPOCH: 960\n",
      "predicted: A: thotem Peveser thear couv rranr thas thaegre ef ther ef ann thol Ratin:\n",
      "yanar fron whets th be nhe\n",
      "LOSS: 2.527200012207031\n",
      "EPOCH: 980\n",
      "predicted: Ae felge and me ope hake,h ther sere and th here be cith thon and.\n",
      "\n",
      "WLRIDTCIES\n",
      "E hether,\n",
      "\n",
      "A'ngl:\n",
      "\n",
      "PLE\n",
      "LOSS: 2.285497131347656\n",
      "EPOCH: 1000\n",
      "predicted: A:\n",
      "The hothun, thy shoughith why sanfen'rs\n",
      "I louss thy to fus yhiten mochose bath hiyed dord soco be,\n",
      "LOSS: 2.3345718383789062\n",
      "EPOCH: 1020\n",
      "predicted: A:\n",
      "Get yorls the woud winl:\n",
      "Men cer I me sorrin cey ande literence,\n",
      "Sol mathy i be the o mortet nend,\n",
      "LOSS: 2.3955784606933594\n",
      "EPOCH: 1040\n",
      "predicted: Aw howed in, rathule and thay that mow, the cor griur il qn mort like wour:\n",
      "\n",
      "KIUREIIITA EAECGA:\n",
      "Thee \n",
      "LOSS: 2.145767822265625\n",
      "EPOCH: 1060\n",
      "predicted: A) IETYU:\n",
      "Hras I to thoug wour say coropt\n",
      "Ar finbene fhe to of on me soy faqc iye ouvyard\n",
      "Augald cour\n",
      "LOSS: 2.144387664794922\n",
      "EPOCH: 1080\n",
      "predicted: AFENIAIO IIPIIRIIIN RDNRRRY:\n",
      "Bis thae kal uf soire bot on ur would roweds thond sing\n",
      "O celd Factor uu\n",
      "LOSS: 2.1364739990234374\n",
      "EPOCH: 1100\n",
      "predicted: A7\n",
      "B\n",
      "dar andeeel laess,\n",
      "\n",
      "BOENILNSNUS:\n",
      "Couuin sre seng, in cord.\n",
      "\n",
      "DENENRK VTI:\n",
      "I whas me osteds goin i\n",
      "LOSS: 2.279105987548828\n",
      "EPOCH: 1120\n",
      "predicted: A.:\n",
      "So peroth noan to mor\n",
      "Gind.\n",
      "\n",
      "OUIILATHET:\n",
      "Me hendret youd that at cards peanm sallmed sore.\n",
      "\n",
      "POAIS\n",
      "LOSS: 2.277829895019531\n",
      "EPOCH: 1140\n",
      "predicted: AZZASIUA:\n",
      "Teos to morme il this hanle anry hor than durgen?\n",
      "\n",
      "RIRIUB:\n",
      "Ar thit thael the the ke the hil\n",
      "LOSS: 2.2408056640625\n",
      "EPOCH: 1160\n",
      "predicted: Avot' the soans were shich thoud of mate your loungess in'd the the yhe-cak leed pore the enle\n",
      "He the\n",
      "LOSS: 2.353316650390625\n",
      "EPOCH: 1180\n",
      "predicted: A6\n",
      "Cout mild fath, much hor o' mime hic.\n",
      "\n",
      "LACIIITD:\n",
      "And sor dritt tigh srour hert it gild hathrer on \n",
      "LOSS: 2.335029754638672\n",
      "EPOCH: 1200\n",
      "predicted: AW ratkoud.\n",
      "\n",
      "SENA AAIDTNO:\n",
      "Anl ass',\n",
      "Th loum.\n",
      "\n",
      "OEAENLIIA:\n",
      "Baw wothaos ywel, fas anledn and thine lild\n",
      "LOSS: 2.3787353515625\n",
      "EPOCH: 1220\n",
      "predicted: Aofdie on and.\n",
      "\n",
      "HAUHS:\n",
      "Bave and theacod themere wows,\n",
      "The tech dacy yoce woolreged-\n",
      "Gald geyer. A'd t\n",
      "LOSS: 2.180608673095703\n",
      "EPOCH: 1240\n",
      "predicted: A!ANIMSSS:\n",
      "Ath is walr it be ige noyd tre srith wusinds:\n",
      "\n",
      "LACIAONA:\n",
      "Go pore sirlor, and word is iy se\n",
      "LOSS: 2.1272442626953123\n",
      "EPOCH: 1260\n",
      "predicted: A/ IWNNT:\n",
      "\n",
      "MIOY:\n",
      "I doaeld, as bpigipter soat fattunt kete me he't to and surl, I sanleds:\n",
      "The pirgevo\n",
      "LOSS: 2.302646026611328\n",
      "EPOCH: 1280\n",
      "predicted: A<\n",
      "I so at to bave she mroth the to konher.\n",
      "\n",
      "LANG ENINA IIY:\n",
      "Nor cith cilgelos soctaw on the kral she\n",
      "LOSS: 2.1041165161132813\n",
      "EPOCH: 1300\n",
      "predicted: A\n",
      "Wor hal's the tith tenk thut It fivenme,\n",
      "Furs bib sonlird btacher foss to got cattirtiine corttise \n",
      "LOSS: 2.1736820983886718\n",
      "EPOCH: 1320\n",
      "predicted: A Houghe hithe,\n",
      "\n",
      "KINOINLO:\n",
      "Wheot you we catias sas nald, whow me as If hithe.\n",
      "\n",
      "Winbkont,\n",
      "Mounror. Fat\n",
      "LOSS: 2.073611145019531\n",
      "EPOCH: 1340\n",
      "predicted: AO:\n",
      "Be rremeseted my and ofser: the herand or let and as the sime whetert.\n",
      "\n",
      "LONRINNTEO:\n",
      "Ubder hay yom\n",
      "LOSS: 2.257296600341797\n",
      "EPOCH: 1360\n",
      "predicted: A fid lowl det Et mud mose lecablematiIgens,\n",
      "Wirly tome the me'et:\n",
      "On san the cewet,\n",
      "IEwouteane tathe\n",
      "LOSS: 2.021562194824219\n",
      "EPOCH: 1380\n",
      "predicted: Az\n",
      "Whe qnper wous miths tiyelencing horgoles: blie, wishans wate is he to marlane, the mrrasant:\n",
      "San \n",
      "LOSS: 2.255050048828125\n",
      "EPOCH: 1400\n",
      "predicted: A/:\n",
      "Muse the bweon, urd afd till te and the bwars i the;\n",
      "And and\n",
      "\n",
      "AINY CTENS:\n",
      "Yor the nath.\n",
      "\n",
      "CARIO:\n",
      "S\n",
      "LOSS: 2.2022218322753906\n",
      "EPOCH: 1420\n",
      "predicted: A0: To, bath and the me;\n",
      "Audting my swass noalt, foare beetkert eed tasth srood beder.\n",
      "\n",
      "TYAUNA CELUUT\n",
      "LOSS: 2.2408924865722657\n",
      "EPOCH: 1440\n",
      "predicted: AT:\n",
      "Firch haue rucles her os I prenler beesell the none.\n",
      "\n",
      "LONIIS:\n",
      "Hathes fis priles, he. Rour aptwem \n",
      "LOSS: 2.1316380310058594\n",
      "EPOCH: 1460\n",
      "predicted: AX srave fathone it hoss of at searser fousib would bang\n",
      "lee, wow thim, thiif ir aunater poripce;\n",
      "\n",
      "BU\n",
      "LOSS: 2.1175071716308596\n",
      "EPOCH: 1480\n",
      "predicted: A.\n",
      "\n",
      "PROSNHOEAORIZUA:\n",
      "Whas the cen hit the wides?\n",
      "We her, thas in thow onane of ould the us dittersy o\n",
      "LOSS: 2.1477482604980467\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5000\n",
    "hidden_size = 200\n",
    "embedding_size = 100\n",
    "n_layers = 4\n",
    "\n",
    "def my_evaluate(start_char, hidden, predict_len = 100):\n",
    "    predicted_chars = []\n",
    "    for i in range(predict_len):\n",
    "        output, hidden, scores = rnn(start_char, hidden)\n",
    "        idx = np.argmax(scores.data.numpy())\n",
    "        predicted_char = all_characters[idx]\n",
    "        predicted_chars.append(predicted_char)\n",
    "        start_char = to_vector(predicted_char)\n",
    "    assert len(predicted_chars) == 100\n",
    "    return \"\".join([str(x) for x in predicted_chars])\n",
    "        \n",
    "rnn = RNN(input_size = n_characters,\n",
    "          embedding_size = embedding_size,\n",
    "          hidden_size = hidden_size, \n",
    "          output_size = n_characters,\n",
    "          n_layers = n_layers)\n",
    "\n",
    "optim = torch.optim.Adam(rnn.parameters(), lr = 0.005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "all_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # re-init hidden and zero the grads\n",
    "    hidden = rnn.init_hidden()\n",
    "    rnn.zero_grad()\n",
    "    inputs, labels = generate_training_set()\n",
    "    # run through the inputs one by one, accumulating a loss\n",
    "    loss = 0\n",
    "    for c in range(seq_len - 1):\n",
    "        output, hidden, scores = rnn(inputs[c], hidden)\n",
    "        loss += criterion(output, labels[c])\n",
    "    loss.backward(retain_graph = True)\n",
    "    optim.step()\n",
    "    all_losses.append(loss.data[0]/seq_len)\n",
    "    if epoch % 20 == 0:\n",
    "        print('EPOCH: {}'.format(epoch))\n",
    "        predicted, _ = evaluate()\n",
    "        print('predicted: {}'.format(predicted))\n",
    "        print('LOSS: {}'.format(loss.data[0]/seq_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
